<!DOCTYPE html>


<html lang="en,zh-CN,default">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>神经网络学习 |  Peter&#39;s Blog</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/images/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss2.xml" title="Peter's Blog" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Computer-Science/Deep Learning/神经网络学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  神经网络学习
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/Computer-Science/Deep%20Learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2020-01-30T09:25:27.000Z" itemprop="datePublished">2020-01-30</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Notes/">Notes</a> / <a class="article-category-link" href="/categories/Notes/Deep-Learning/">Deep Learning</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">2.9k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">15 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/lightgallery.js/1.0.1/css/lightgallery.min.css" /><div class=".article-gallery"><h1 id="神经网络-学习笔记"><a href="#神经网络-学习笔记" class="headerlink" title="神经网络 学习笔记"></a>神经网络 学习笔记</h1><p>学习资源: <a href="">neuralnetworksanddeeplearning.com</a></p>
<h2 id="Chapter-1、2"><a href="#Chapter-1、2" class="headerlink" title="Chapter 1、2"></a>Chapter 1、2</h2><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h3><ul>
<li><p>activation function 激活函数&#x2F;传递函数   定义了该节点在给定的输入或输入的集合下的输出，比如 sigmoid function</p>
</li>
<li><p>perceptron model 感知器：输入 $0,1$ 二值函数，输出 $0,1$ 二值函数</p>
</li>
<li><p>sigmoid neurons  S 神经元  </p>
<p>​	中间部分更精细，$\displaystyle\sigma(z)&#x3D;\frac{1}{1+e^{-z}}$ 输入二值，输出为 $[0,1]$ 之间的数，极限性质和感知器一样</p>
</li>
<li><p>feedforward neural networks 前馈神经网络</p>
<p>​	没有反馈的神经网络，由输入层(input layer)、隐藏层(hidden layer)、输出层(output layer)构成</p>
</li>
</ul>
<p>$ \ $ <span id="more"></span></p>
<h3 id="1-2-stochastic-gradient-descent-随机梯度下降"><a href="#1-2-stochastic-gradient-descent-随机梯度下降" class="headerlink" title="1.2 stochastic gradient descent 随机梯度下降"></a>1.2 stochastic gradient descent 随机梯度下降</h3><h4 id="1-2-1-参数"><a href="#1-2-1-参数" class="headerlink" title="1.2.1 参数"></a>1.2.1 参数</h4><ul>
<li>$x$ ：输入的训练集（一个矩阵，每一列是一次单独的输入）,  $y$ ：最终输出，$y&#x3D;w\cdot x+b$</li>
<li>$w$：weights，由矩阵组成的向量。$w_j$ 是一个 $m\times n$ 型矩阵，连接 $j-1$ 层和 $j$ 层，其中 $j-1$ 层有 $n$ 个神经元，$ j $ 层有 $m$ 个神经元</li>
<li>$b$ ：biases，由 $ m\times 1$ 型矩阵组成的向量。 $b_k$ 是一个$m\times 1$ 型矩阵，连接 $j-1$ 层和 $j$ 层，其中第 $j$ 层有 $m$ 个神经元</li>
<li>Cost function ： $ C(w,b)&#x3D;\displaystyle\frac{1}{2n}\sum_x |y(x)-a |^2&#x3D;\frac{1}{2n}\sum_{i&#x3D;1}^{n}[y(x^{(i)})-a^{(i)}]^2 $</li>
</ul>
<h4 id="1-2-2-方法"><a href="#1-2-2-方法" class="headerlink" title="1.2.2 方法"></a>1.2.2 方法</h4><ul>
<li>对于整个训练集，用随机梯度下降法使损失函数尽可能小</li>
<li>先将训练集打乱，再按给定的小批训练集大小等间隔取小批训练集（相当于随机取样），每次选择更优的 $w,b$ 使损失函数一步步减小</li>
</ul>
<h4 id="1-2-3-保证每次都获得更优的-w-b-："><a href="#1-2-3-保证每次都获得更优的-w-b-：" class="headerlink" title="1.2.3 保证每次都获得更优的 $w,b$："></a>1.2.3 保证每次都获得更优的 $w,b$：</h4><p>1. </p>
<p>$$<br>\displaystyle \nabla C&#x3D;(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T , \quad \vec v_1\equiv v_1\equiv w,\vec v_2\equiv v_2\equiv b\ \displaystyle\Delta C\approx  \frac{\partial C}{\partial w}\Delta v_1+\frac{\partial C}{\partial b}\Delta v_2&#x3D;\Delta v\cdot \nabla C \<br>&#x3D;&gt; take\quad \Delta v&#x3D;(\Delta v_1,\Delta v_2)&#x3D;-\eta \nabla C,\quad \eta&gt;0\<br>\begin{aligned}<br>\therefore C&amp;:&#x3D;C+\Delta C&#x3D;C-\eta |\nabla C |^2\<br>v_1&amp; :&#x3D;v_1-\eta\cdot\frac{\partial C}{\partial v_1}\<br>v_2&amp;:&#x3D;v_2-\eta\cdot\frac{\partial C}{\partial v_2s}<br>\end{aligned}<br>$$</p>
<ol start="2">
<li>分量式：</li>
</ol>
<p>$$<br>\begin{aligned}<br>w_{jk}&amp;:&#x3D;w_{jk}-\eta\cdot \frac{\partial C}{\partial w_{jk}}&#x3D;w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i&#x3D;1}^m \frac{\partial C}{\partial w_{jk}^{(i)}} \<br>b_{k}&amp;:&#x3D;b_{ k}-\eta\cdot \frac{\partial C}{\partial b_{k}}&#x3D;b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i&#x3D;1}^m \frac{\partial C}{\partial b_{k}^{(i)}}<br>\end{aligned}<br>$$</p>
<h2 id="2-BP算法（反向传播-back-propagate-）"><a href="#2-BP算法（反向传播-back-propagate-）" class="headerlink" title="2 BP算法（反向传播 back propagate ）"></a>2 BP算法（反向传播 back propagate ）</h2><p>目的：快速计算 $\displaystyle \frac{\partial C}{\partial w},\frac{\partial C}{\partial b}$ .</p>
<h3 id="2-1-原理："><a href="#2-1-原理：" class="headerlink" title="2.1 原理："></a>2.1 原理：</h3><p>最终结果产生的偏差是由于每个神经元产生的误差叠加起来造成的。通过前向反馈得到的结果若与正确结果有偏差，则从此偏差反向推导，可得到各层神经元产生的误差。</p>
<h3 id="2-2-符号表示"><a href="#2-2-符号表示" class="headerlink" title="2.2 符号表示"></a>2.2 符号表示</h3><ul>
<li><p>$w_{jk}^l$ ：连接第 $l-1$层第 $k$ 个神经元和第 $l$ 层第 $j$ 个神经元的权重</p>
</li>
<li><p>$w^l$ ：连接第 $l-1$ 和第 $l$ 层网络的权重矩阵</p>
</li>
<li><p>$b_j^l$ ：第 $l$ 层第 $j$ 个神经元的 bias</p>
</li>
<li><p>$z_j^l$ ：第 $l$ 层网络的带权输入  $z_j^l&#x3D;\displaystyle\sum_k w_{jk}^l a_k^{l-1}+b_j^l$，$z^l&#x3D;w^l a^{l-1}+b^l$</p>
</li>
<li><p>$\delta_j^l$ ：第 $l$ 层第 $j$ 个神经元产生的误差,，定义 $\displaystyle \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}$</p>
</li>
<li><p>$a_j^l$ ：第 $l$ 层网络的第 $j$ 个神经元的激活量，$a_j^l&#x3D;\sigma(z_j^l)$</p>
</li>
<li><p>$C$ ：损失函数，用小批量输入的平均值近似。</p>
</li>
</ul>
<h3 id="2-3-重要的关系公式"><a href="#2-3-重要的关系公式" class="headerlink" title="2.3 重要的关系公式"></a>2.3 重要的关系公式</h3><ol start="0">
<li><p>注：推导中的除法 ‘’$\bigg &#x2F;$ ‘’、乘法 $\odot$ （ Hadamard Product）均为<strong>elementwise</strong> </p>
</li>
<li><p>$$<br>C&#x3D;\displaystyle \frac{1}{2n}\sum_x |y(x)-a^L(x)|^2&#x3D;\frac{1}{n}\sum_x ^n C(x)\approx\frac{1}{m}\sum_x^m C(x)<br>\ C(x)&#x3D;\frac{1}{2}|y-a^L(x)|^2&#x3D;\frac{1}{2}\sum_i \big(y_i-a_i^L(x)\big)^2<br>$$</p>
</li>
<li><p>$$<br>a_j^l&#x3D;\sigma\bigg(\sum_k w_{jk}^l a_k^{l-1}+b_j^l\bigg)<br>\ a^l&#x3D;\sigma\bigg(w^l a^{l-1}+b^l\bigg)<br>$$</p>
</li>
<li><p>$$<br>\delta^l&#x3D;\frac{\partial C}{\partial b^l}\bigg&#x2F; \frac{\partial z^l}{\partial b^l}&#x3D;\frac{\partial C}{\partial b^l},\qquad \frac{\partial C}{\partial b^l}\triangleq(\frac{\partial C}{\partial b_1^l},\cdots,\frac{\partial C}{\partial b_{N(l)}^l})^T \tag{1}<br>$$</p>
</li>
<li><p>$$<br>\delta_j^l&#x3D;\frac{\partial C}{\partial w_{jk}^l}\bigg &#x2F;\frac{\partial z^l}{\partial w_{jk}^l}&#x3D;\frac{\partial C}{\partial w_{jk}^l}\big&#x2F; a_k^{l-1}<br>$$</p>
</li>
</ol>
<p>$$<br>i .e. \qquad \frac{\partial C}{\partial w_{jk}^l}&#x3D;a_{k}^{l-1}\cdot  \delta_j^l \qquad &#x3D;&gt;\nabla C_{w^l}&#x3D;\delta^l\cdot (a^{l-1})^T \tag{2}<br>$$</p>
<p>5.<br>   $$<br>   \delta^L&#x3D;\frac{\partial C}{\partial a^L}\odot \frac{\partial a^L}{\partial z^L}&#x3D;\frac{\partial C}{\partial a^L}\odot\sigma’(z^L)&#x3D;(a^L-y) \odot \sigma’(z^L)\tag{3}<br>   $$</p>
<ol start="6">
<li><p><strong>Move error backward:</strong><br>$$<br>\begin{eqnarray}<br>z^{l+1}<em>k &#x3D; \sum_j w^{l+1}</em>{kj} a^l_j +b^{l+1}<em>k &#x3D; \sum_j w^{l+1}</em>{kj} \sigma(z^l_j) +b^{l+1}_k<br>&#x3D;&gt;\frac{\partial z^{l+1}<em>k}{\partial z^l_j} &#x3D; w^{l+1}</em>{kj} \sigma’(z^l_j)<br>\end{eqnarray}<br>$$</p>
<p>$$<br>\delta_j^l&#x3D;\sum_k \frac{\partial C}{\partial z_k^{l+1}}\cdot \frac{\partial z_k^{l+1}}{\partial z_j^l}&#x3D;\sum_{k&#x3D;1}^{N(l)}\delta_k^{l+1}\cdot w_{kj}^{l+1}\cdot \sigma’(z_j^l)<br>$$</p>
<p>$$<br>&#x3D;&gt; \delta^l&#x3D;\big((w^{l+1})^T\sigma’(z^l)\big)\odot \delta^{l+1}\tag{4}<br>$$</p>
</li>
</ol>
<h3 id="2-4-实现步骤"><a href="#2-4-实现步骤" class="headerlink" title="2.4 实现步骤"></a>2.4 实现步骤</h3><ol>
<li><p>feedforward :  得到 $z^L$ 和相应的 $a^L$<br>$$<br>a^{x,l} &#x3D; \sigma(z^{x,l})<br>$$</p>
</li>
<li><p>output error :<br>$$<br>\delta^{x,L} &#x3D; \nabla_a C_x \odot \sigma’(z^{x,L})<br>$$</p>
</li>
<li><p>error back propagate :<br>$$<br>\delta^{x,l} &#x3D; ((w^{l+1})^T \delta^{x,l+1})<br>  \odot \sigma’(z^{x,l})<br>$$</p>
</li>
<li><p>update weights and biases :<br>$$<br>\begin{aligned}<br>w_{jk}&amp;:&#x3D;w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i&#x3D;1}^m \frac{\partial C}{\partial w_{jk}^{(x_i)}}&#x3D;w_{jk}-\eta\cdot\frac{1}{m}\sum_{i&#x3D;1}^m \big(a_k^{l-1}\delta_j^{l}\big)^{(x_i)}\<br>b_{k}&amp;:&#x3D;b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i&#x3D;1}^m \frac{\partial C}{\partial b_{k}^{(x_i)}}&#x3D;b_{k}-\eta\cdot\frac{1}{m}\sum_{i&#x3D;1}^m \big(\delta_k^{l}\big)^{(x_i)}<br>\end{aligned}<br>$$</p>
<p>即：<br>$$<br>w^l \rightarrow<br>  w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T<br> \b^l \rightarrow b^l-\frac{\eta}{m}<br>  \sum_x \delta^{x,l}<br>$$</p>
</li>
</ol>
<h3 id="2-5-代码实现"><a href="#2-5-代码实现" class="headerlink" title="2.5 代码实现"></a>2.5 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sizes</span>):</span><br><span class="line">        <span class="comment"># 生成神经网络 </span></span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(sizes[:-<span class="number">1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment"># 激活神经元，向后面的层产生反馈</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a) + b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span><br><span class="line"><span class="params">            test_data=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 随机梯度下降 Stochastic Gradient Descent</span></span><br><span class="line">        <span class="comment"># epochs 训练次数, mini_batch_size 每次选取的小训练集大小, eta 学习率 由自己设定</span></span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            test_data = <span class="built_in">list</span>(test_data)  </span><br><span class="line">            n_test = <span class="built_in">len</span>(test_data)</span><br><span class="line">        training_data = <span class="built_in">list</span>(training_data)</span><br><span class="line">        n = <span class="built_in">len</span>(training_data)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k + mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, mini_batch_size)]  </span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;&quot;</span>.<span class="built_in">format</span>(j,</span><br><span class="line">                                                    self.evaluate(test_data), n_test))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;0&#125; complete&quot;</span>.<span class="built_in">format</span>(j))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span><br><span class="line">        <span class="comment"># 更新 w 和 b</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb + dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw + dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w - (eta / <span class="built_in">len</span>(mini_batch)) * nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b - (eta / <span class="built_in">len</span>(mini_batch)) * nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 反向传播算法</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x]  <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = []  <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation) + b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[-<span class="number">1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[-<span class="number">1</span>])<span class="comment"># \delta^L</span></span><br><span class="line">        nabla_b[-<span class="number">1</span>] = delta</span><br><span class="line">        nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l + <span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l - <span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, test_data</span>):</span><br><span class="line">       <span class="comment"># 返回输出结果和训练数据集的正确结果相同的个数</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">int</span>(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cost_derivative</span>(<span class="params">self, output_activations, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations - y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Miscellaneous functions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment"># The sigmoid function.</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment"># Derivative of the sigmoid function.</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z) * (<span class="number">1</span> - sigmoid(z))</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="Chapter-3"><a href="#Chapter-3" class="headerlink" title="Chapter 3"></a>Chapter 3</h2><h3 id="3-1-交叉熵"><a href="#3-1-交叉熵" class="headerlink" title="3.1 交叉熵"></a>3.1 交叉熵</h3><p>对错误的惩罚力度加大，学习速率更快</p>
<h3 id="3-2-Softmax"><a href="#3-2-Softmax" class="headerlink" title="3.2 Softmax"></a>3.2 Softmax</h3><p>输出转化为概率分布</p>
<h3 id="3-3-其他损失函数"><a href="#3-3-其他损失函数" class="headerlink" title="3.3 其他损失函数"></a>3.3 其他损失函数</h3><h3 id="3-4-调整超参数的方法"><a href="#3-4-调整超参数的方法" class="headerlink" title="3.4 调整超参数的方法"></a>3.4 调整超参数的方法</h3><ul>
<li><p>正则化<br>$$<br>C&#x3D;C_0+\frac{\lambda}{n}|w|^2<br>$$<br>不能提升 $C_0$ ，权重 $w$ 会下降</p>
</li>
<li><p>初始化</p>
<p>考虑 $z&#x3D;\sum_{j&#x3D;1}^n w_jz_j+b$, 初始化 $w\sim N(0,1&#x2F;n)$</p>
</li>
</ul>
<h2 id="Chapter-4"><a href="#Chapter-4" class="headerlink" title="Chapter 4"></a>Chapter 4</h2><p>神经网络可以计算任何函数。</p>
<h2 id="Chapter-5"><a href="#Chapter-5" class="headerlink" title="Chapter 5"></a>Chapter 5</h2><ul>
<li><p>vanishing gradient</p>
</li>
<li><p>exploding gradient</p>
</li>
</ul>
<h2 id="Chapter-6-CNN"><a href="#Chapter-6-CNN" class="headerlink" title="Chapter 6 CNN"></a>Chapter 6 CNN</h2><h3 id="6-1-架构"><a href="#6-1-架构" class="headerlink" title="6.1 架构"></a>6.1 架构</h3><ul>
<li>卷积层<ul>
<li>共享权重</li>
<li>提取特征</li>
</ul>
</li>
<li>池化层<ul>
<li>减少参量</li>
</ul>
</li>
<li>全连层<ul>
<li>二维压缩成一维，方便输出</li>
</ul>
</li>
<li>Softmax层<ul>
<li>输出概率分布</li>
</ul>
</li>
</ul>
<h3 id="6-2-代码：-with-pytorch"><a href="#6-2-代码：-with-pytorch" class="headerlink" title="6.2 代码： with pytorch"></a>6.2 代码： with pytorch</h3><h4 id="6-2-1-数据载入"><a href="#6-2-1-数据载入" class="headerlink" title="6.2.1 数据载入"></a>6.2.1 数据载入</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./&#x27;</span>,    <span class="comment"># 保存或者提取位置</span></span><br><span class="line">    train=<span class="literal">True</span>, </span><br><span class="line">    <span class="comment"># 转换 PIL.Image or numpy.ndarray</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=DOWNLOAD_MNIST,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># print(train_data.data.shape)</span></span><br><span class="line"><span class="comment">#train_data.data : (60000,28,28)</span></span><br><span class="line"><span class="comment"># train_data.targets : (60000)</span></span><br><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># test_data 包括：test_data.data 和 test_data.targets</span></span><br><span class="line"><span class="comment">#test_data.data : (10000,28,28)</span></span><br><span class="line"><span class="comment">#test_data.targets : (10000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></span><br><span class="line">train_batches = Data.DataLoader(</span><br><span class="line">    dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(train_batches.dataset.targets.shape</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试前2000个</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[</span><br><span class="line">    :<span class="number">2000</span>] / <span class="number">255.0</span>  <span class="comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></span><br><span class="line">test_y = test_data.targets[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>



<h4 id="6-2-2-卷积神经网络构建"><a href="#6-2-2-卷积神经网络构建" class="headerlink" title="6.2.2 卷积神经网络构建"></a>6.2.2 卷积神经网络构建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(  <span class="comment"># input shape (1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">1</span>,      <span class="comment"># input height</span></span><br><span class="line">                out_channels=<span class="number">16</span>,    <span class="comment"># n_filters i.e. number of features</span></span><br><span class="line">                kernel_size=<span class="number">5</span>,      <span class="comment"># filter size</span></span><br><span class="line">                stride=<span class="number">1</span>,           <span class="comment"># filter movement/step</span></span><br><span class="line">                <span class="comment"># 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1</span></span><br><span class="line">                padding=<span class="number">2</span>,</span><br><span class="line">            ),      <span class="comment"># -&gt; (16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),    <span class="comment"># activation</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),    <span class="comment"># -&gt; (16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(  <span class="comment"># input shape (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># output shape (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),  <span class="comment"># activation</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),  <span class="comment"># output shape (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># fully connected layer, output 10 classes</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">100</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)   <span class="comment"># 展平多维的卷积图成 (batch_size, 32 * 7 * 7)</span></span><br><span class="line">        x = torch.sigmoid(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="6-2-3-反向传播、训练神经网络"><a href="#6-2-3-反向传播、训练神经网络" class="headerlink" title="6.2.3 反向传播、训练神经网络"></a>6.2.3 反向传播、训练神经网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cnn = CNN()</span><br><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) <span class="comment">#adam算法</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()  <span class="comment"># Softmax–Log–NLLLoss，自动输出log-softmax</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># 分配 batch data, normalize x when iterate train_loader</span></span><br><span class="line">        <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_batches):  <span class="comment"># enumerate 添加索引可便历</span></span><br><span class="line">            <span class="comment"># print(b_x.shape)--&gt;(50,1,28,28)</span></span><br><span class="line">            <span class="comment"># print(b_y.shape)--&gt;(50)</span></span><br><span class="line">            <span class="comment"># 1200组</span></span><br><span class="line">            output = cnn(b_x)               <span class="comment"># cnn output</span></span><br><span class="line">            loss = loss_func(output, b_y)   <span class="comment"># cross entropy loss</span></span><br><span class="line">            optimizer.zero_grad()           <span class="comment"># clear gradients for this training step</span></span><br><span class="line">            loss.backward()                 <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">            optimizer.step()                <span class="comment"># apply gradients</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                test_output = cnn(test_x)</span><br><span class="line">                pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[</span><br><span class="line">                    <span class="number">1</span>].data.numpy()  <span class="comment"># [0]:一行中的最大值,[1]:最大值的索引</span></span><br><span class="line">                accuracy = <span class="built_in">float</span>((pred_y == test_y.data.numpy()).astype(</span><br><span class="line">                    <span class="built_in">int</span>).<span class="built_in">sum</span>()) / <span class="built_in">float</span>(test_y.size(<span class="number">0</span>))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch, <span class="string">&#x27;| train loss: %.4f&#x27;</span> %</span><br><span class="line">                      loss.data.numpy(), <span class="string">&#x27;| test accuracy: %.2f&#x27;</span> % accuracy)</span><br><span class="line">    torch.save(cnn.state_dict(), <span class="string">&quot;cnn_net_params.pkl&quot;</span>)</span><br><span class="line"></span><br><span class="line">train()</span><br></pre></td></tr></table></figure>



<h4 id="6-2-4-参数存储与加载"><a href="#6-2-4-参数存储与加载" class="headerlink" title="6.2.4 参数存储与加载"></a>6.2.4 参数存储与加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.save(cnn.state_dict(), <span class="string">&quot;cnn_net_params.pkl&quot;</span>)<span class="comment">#保存训练好的参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line">cnn.load_state_dict(torch.load(<span class="string">&#x27;cnn_net_params.pkl&#x27;</span>))<span class="comment">#加载参数</span></span><br></pre></td></tr></table></figure>









<p>其他：关于深度学习和图像处理卷积的定义问题：深度学习中conv2d卷积层其实是无所谓是否翻转的，因为所有的weights也就是kernel其实是随机初始化的。那么每次的更新迭代都是为了去寻找一个最合适的kernel，所以是否翻转也变的无关紧要了。</p>
</div><script src="https://cdn.jsdelivr.net/lightgallery.js/1.0.1/js/lightgallery.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/Mathematics/Mathematical-Modeling/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Mathematic Modeling Notes
          
        </div>
      </a>
    
    
      <a href="/Computer-Science/Python-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python Notes</div>
      </a>
    
  </nav>

  
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2023
        <i class="ri-heart-fill heart_icon"></i> Longqian Huang
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/cat.png" alt="Peter&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/news">近闻</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/publications">发表</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/projects">项目</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/notes">笔记</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/gallery">画廊</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>来，请俺喝杯咖啡！</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>