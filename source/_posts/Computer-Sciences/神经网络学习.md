---
title: 神经网络学习
date: 2020-01-30 17:25:27
tags: notes
categories:
 - Computer Science
 - Machine Learning
 - Neural Networks
---

# 神经网络 学习笔记

学习资源: [neuralnetworksanddeeplearning.com]()



## Chapter 1、2

### 1.1 基本概念

- activation function 激活函数/传递函数   定义了该节点在给定的输入或输入的集合下的输出，比如 sigmoid function

- perceptron model 感知器：输入 $0,1$ 二值函数，输出 $0,1$ 二值函数

- sigmoid neurons  S 神经元  

  ​	中间部分更精细，$\displaystyle\sigma(z)=\frac{1}{1+e^{-z}}$ 输入二值，输出为 $[0,1]$ 之间的数，极限性质和感知器一样

- feedforward neural networks 前馈神经网络

  ​	没有反馈的神经网络，由输入层(input layer)、隐藏层(hidden layer)、输出层(output layer)构成

$ \\ $ <!--more-->



### 1.2 stochastic gradient descent 随机梯度下降

#### 1.2.1 参数

- $x$ ：输入的训练集（一个矩阵，每一列是一次单独的输入）,  $y$ ：最终输出，$y=w\cdot x+b$
- $w$：weights，由矩阵组成的向量。$w_j$ 是一个 $m\times n$ 型矩阵，连接 $j-1$ 层和 $j$ 层，其中 $j-1$ 层有 $n$ 个神经元，$ j $ 层有 $m$ 个神经元
- $b$ ：biases，由 $ m\times 1$ 型矩阵组成的向量。 $b_k$ 是一个$m\times 1$ 型矩阵，连接 $j-1$ 层和 $j$ 层，其中第 $j$ 层有 $m$ 个神经元
- Cost function ： $ C(w,b)=\displaystyle\frac{1}{2n}\sum_x \|y(x)-a \|^2=\frac{1}{2n}\sum_{i=1}^{n}[y(x^{(i)})-a^{(i)}]^2 $



#### 1.2.2 方法

- 对于整个训练集，用随机梯度下降法使损失函数尽可能小
- 先将训练集打乱，再按给定的小批训练集大小等间隔取小批训练集（相当于随机取样），每次选择更优的 $w,b$ 使损失函数一步步减小



#### 1.2.3 保证每次都获得更优的 $w,b$：

1. 

$$
\displaystyle \nabla C=(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T , \quad \vec v_1\equiv v_1\equiv w,\vec v_2\equiv v_2\equiv b\\ \displaystyle\Delta C\approx  \frac{\partial C}{\partial w}\Delta v_1+\frac{\partial C}{\partial b}\Delta v_2=\Delta v\cdot \nabla C \\ 
=> take\quad \Delta v=(\Delta v_1,\Delta v_2)=-\eta \nabla C,\quad \eta>0\\
\begin{aligned}
\therefore C&:=C+\Delta C=C-\eta \|\nabla C \|^2\\
v_1& :=v_1-\eta\cdot\frac{\partial C}{\partial v_1}\\
v_2&:=v_2-\eta\cdot\frac{\partial C}{\partial v_2s}
\end{aligned}
$$

2. 分量式：

$$
\begin{aligned}
w_{jk}&:=w_{jk}-\eta\cdot \frac{\partial C}{\partial w_{jk}}=w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial w_{jk}^{(i)}} \\
b_{k}&:=b_{ k}-\eta\cdot \frac{\partial C}{\partial b_{k}}=b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial b_{k}^{(i)}}
\end{aligned}
$$





## 2 BP算法（反向传播 back propagate ）

目的：快速计算 $\displaystyle \frac{\partial C}{\partial w},\frac{\partial C}{\partial b}$ .

### 2.1 原理：

最终结果产生的偏差是由于每个神经元产生的误差叠加起来造成的。通过前向反馈得到的结果若与正确结果有偏差，则从此偏差反向推导，可得到各层神经元产生的误差。



### 2.2 符号表示

- $w_{jk}^l$ ：连接第 $l-1$层第 $k$ 个神经元和第 $l$ 层第 $j$ 个神经元的权重
- $w^l$ ：连接第 $l-1$ 和第 $l$ 层网络的权重矩阵
- $b_j^l$ ：第 $l$ 层第 $j$ 个神经元的 bias
- $z_j^l$ ：第 $l$ 层网络的带权输入  $z_j^l=\displaystyle\sum_k w_{jk}^l a_k^{l-1}+b_j^l$，$z^l=w^l a^{l-1}+b^l$
- $\delta_j^l$ ：第 $l$ 层第 $j$ 个神经元产生的误差,，定义 $\displaystyle \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}$

- $a_j^l$ ：第 $l$ 层网络的第 $j$ 个神经元的激活量，$a_j^l=\sigma(z_j^l)$
- $C$ ：损失函数，用小批量输入的平均值近似。



### 2.3 重要的关系公式

0. 注：推导中的除法 ''$\bigg /$ ''、乘法 $\odot$ （ Hadamard Product）均为**elementwise** 

1. $$
   C=\displaystyle \frac{1}{2n}\sum_x \|y(x)-a^L(x)\|^2=\frac{1}{n}\sum_x ^n C(x)\approx\frac{1}{m}\sum_x^m C(x)
   \\ C(x)=\frac{1}{2}\|y-a^L(x)\|^2=\frac{1}{2}\sum_i \big(y_i-a_i^L(x)\big)^2
   $$



2. $$
   a_j^l=\sigma\bigg(\sum_k w_{jk}^l a_k^{l-1}+b_j^l\bigg)
   \\ a^l=\sigma\bigg(w^l a^{l-1}+b^l\bigg)
   $$



3. $$
   \delta^l=\frac{\partial C}{\partial b^l}\bigg/ \frac{\partial z^l}{\partial b^l}=\frac{\partial C}{\partial b^l},\qquad \frac{\partial C}{\partial b^l}\triangleq(\frac{\partial C}{\partial b_1^l},\cdots,\frac{\partial C}{\partial b_{N(l)}^l})^T \tag{1}
   $$

   



4. $$
   \delta_j^l=\frac{\partial C}{\partial w_{jk}^l}\bigg /\frac{\partial z^l}{\partial w_{jk}^l}=\frac{\partial C}{\partial w_{jk}^l}\big/ a_k^{l-1}
   $$


$$
i .e. \qquad \frac{\partial C}{\partial w_{jk}^l}=a_{k}^{l-1}\cdot  \delta_j^l \qquad =>\nabla C_{w^l}=\delta^l\cdot (a^{l-1})^T \tag{2}
$$


5. 
   $$
   \delta^L=\frac{\partial C}{\partial a^L}\odot \frac{\partial a^L}{\partial z^L}=\frac{\partial C}{\partial a^L}\odot\sigma'(z^L)=(a^L-y) \odot \sigma'(z^L)\tag{3}
   $$
   



6. **Move error backward:**
   $$
   \begin{eqnarray}
   z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k
   =>\frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j)
   \end{eqnarray}
   $$

   $$
   \delta_j^l=\sum_k \frac{\partial C}{\partial z_k^{l+1}}\cdot \frac{\partial z_k^{l+1}}{\partial z_j^l}=\sum_{k=1}^{N(l)}\delta_k^{l+1}\cdot w_{kj}^{l+1}\cdot \sigma'(z_j^l)
   $$

   
   $$
   => \delta^l=\big((w^{l+1})^T\sigma'(z^l)\big)\odot \delta^{l+1}\tag{4}
   $$
   



### 2.4 实现步骤

1. feedforward :  得到 $z^L$ 和相应的 $a^L$
   $$
   a^{x,l} = \sigma(z^{x,l})
   $$
   
2. output error :
   $$
   \delta^{x,L} = \nabla_a C_x \odot \sigma'(z^{x,L})
   $$
   
3. error back propagate :  
   $$
   \delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1})
     \odot \sigma'(z^{x,l})
   $$
   

   
4. update weights and biases : 
   $$
   \begin{aligned}
   w_{jk}&:=w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial w_{jk}^{(x_i)}}=w_{jk}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \big(a_k^{l-1}\delta_j^{l}\big)^{(x_i)}\\
   b_{k}&:=b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial b_{k}^{(x_i)}}=b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \big(\delta_k^{l}\big)^{(x_i)}
   \end{aligned}
   $$
   

   即：
   $$
   w^l \rightarrow
     w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T
    \\b^l \rightarrow b^l-\frac{\eta}{m}
     \sum_x \delta^{x,l}
   $$
   

   





### 2.5 代码实现

``` python
import random
import numpy as np


class Network(object):

    def __init__(self, sizes):
        # 生成神经网络 
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]

    def feedforward(self, a):
        # 激活神经元，向后面的层产生反馈
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a) + b)
        return a

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        # 随机梯度下降 Stochastic Gradient Descent
        # epochs 训练次数, mini_batch_size 每次选取的小训练集大小, eta 学习率 由自己设定
        if test_data:
            test_data = list(test_data)  
            n_test = len(test_data)
        training_data = list(training_data)
        n = len(training_data)
        
        for j in range(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k + mini_batch_size]
                for k in range(0, n, mini_batch_size)]  
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print("Epoch {0}: {1} / {2}".format(j,
                                                    self.evaluate(test_data), n_test))

            else:
                print("Epoch {0} complete".format(j))

    def update_mini_batch(self, mini_batch, eta):
        # 更新 w 和 b
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w - (eta / len(mini_batch)) * nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b - (eta / len(mini_batch)) * nb
                       for b, nb in zip(self.biases, nabla_b)]

    def backprop(self, x, y):
        # 反向传播算法
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x]  # list to store all the activations, layer by layer
        zs = []  # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation) + b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])# \delta^L
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())

        for l in range(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
       # 返回输出结果和训练数据集的正确结果相同的个数
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results) 

    def cost_derivative(self, output_activations, y):
        """Return the vector of partial derivatives \partial C_x /
        \partial a for the output activations."""
        return (output_activations - y)

# Miscellaneous functions

def sigmoid(z):
    # The sigmoid function.
    return 1.0 / (1.0 + np.exp(-z))


def sigmoid_prime(z):
    # Derivative of the sigmoid function.
    return sigmoid(z) * (1 - sigmoid(z))

```





## Chapter 3

### 3.1 交叉熵

对错误的乘法力度加大，学习速率更快

### 3.2 Softmax

输出转化为概略分布

### 3.3 其他损失函数

### 3.4 调整超参数的方法





## Chapter 4

神经网络可以计算任何函数。



## Chapter 5

- vanishing gradient

- exploding gradient

  

## Chapter 6 CNN

### 6.1 架构

- 卷积层
  - 共享权重
  - 提取特征
- 池化层
  - 减少参量
- 全连层
  - 二维压缩成一维，方便输出
- Softmax层
  - 输出概率分布



### 6.2 代码： with pytorch

#### 6.2.1 数据载入

```python
train_data = torchvision.datasets.MNIST(
    root='./',    # 保存或者提取位置
    train=True, 
    # 转换 PIL.Image or numpy.ndarray
    transform=torchvision.transforms.ToTensor(),
    download=DOWNLOAD_MNIST,
)
# print(train_data.data.shape)
#train_data.data : (60000,28,28)
# train_data.targets : (60000)
test_data = torchvision.datasets.MNIST(root='./', train=False)
# test_data 包括：test_data.data 和 test_data.targets
#test_data.data : (10000,28,28)
#test_data.targets : (10000)

# 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)
train_batches = Data.DataLoader(
    dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)
# print(train_batches.dataset.targets.shape

# 测试前2000个
test_x = torch.unsqueeze(test_data.data, dim=1).type(torch.FloatTensor)[
    :2000] / 255.0  # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)
test_y = test_data.targets[:2000]
```



#### 6.2.2 卷积神经网络构建

```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)
            nn.Conv2d(
                in_channels=1,      # input height
                out_channels=16,    # n_filters i.e. number of features
                kernel_size=5,      # filter size
                stride=1,           # filter movement/step
                # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1
                padding=2,
            ),      # -> (16, 28, 28)
            nn.ReLU(),    # activation
            nn.MaxPool2d(kernel_size=2),    # -> (16, 14, 14)
        )
        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)
            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)
            nn.ReLU(),  # activation
            nn.MaxPool2d(2),  # output shape (32, 7, 7)
        )
        # fully connected layer, output 10 classes
        self.fc1 = nn.Linear(32 * 7 * 7, 100)
        self.fc2 = nn.Linear(100, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)
        x = torch.sigmoid(self.fc1(x))
        x = self.fc2(x)
        return x

```



#### 6.2.3 反向传播、训练神经网络

```python
cnn = CNN()
optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) #adam算法
loss_func = nn.CrossEntropyLoss()  # Softmax–Log–NLLLoss，自动输出log-softmax

def train():
    for epoch in range(epochs):
        # 分配 batch data, normalize x when iterate train_loader
        for step, (b_x, b_y) in enumerate(train_batches):  # enumerate 添加索引可便历
            # print(b_x.shape)-->(50,1,28,28)
            # print(b_y.shape)-->(50)
            # 1200组
            output = cnn(b_x)               # cnn output
            loss = loss_func(output, b_y)   # cross entropy loss
            optimizer.zero_grad()           # clear gradients for this training step
            loss.backward()                 # backpropagation, compute gradients
            optimizer.step()                # apply gradients
            if step % 100 == 0:
                test_output = cnn(test_x)
                pred_y = torch.max(test_output, 1)[
                    1].data.numpy()  # [0]:一行中的最大值,[1]:最大值的索引
                accuracy = float((pred_y == test_y.data.numpy()).astype(
                    int).sum()) / float(test_y.size(0))
                print('Epoch: ', epoch, '| train loss: %.4f' %
                      loss.data.numpy(), '| test accuracy: %.2f' % accuracy)
    torch.save(cnn.state_dict(), "cnn_net_params.pkl")

train()
```



#### 6.2.4 参数存储与加载

```python
torch.save(cnn.state_dict(), "cnn_net_params.pkl")#保存训练好的参数


cnn = CNN()
cnn.load_state_dict(torch.load('cnn_net_params.pkl'))#加载参数
```









其他：关于深度学习和图像处理卷积的定义问题：深度学习中conv2d卷积层其实是无所谓是否翻转的，因为所有的weights也就是kernel其实是随机初始化的。那么每次的更新迭代都是为了去寻找一个最合适的kernel，所以是否翻转也变的无关紧要了。

