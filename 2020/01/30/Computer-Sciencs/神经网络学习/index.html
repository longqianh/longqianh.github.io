<!DOCTYPE html>


<html lang="en">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    神经网络学习 |  Peter&#39;s Blog
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/images/favicon.png" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  
  

  

</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content on">
      <section class="outer">
  <article id="post-Computer-Sciencs/神经网络学习" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  神经网络学习
</h1>
 

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/01/30/Computer-Sciencs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2020-01-30T09:25:27.000Z" itemprop="datePublished">2020-01-30</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Computer-Science/">Computer Science</a> / <a class="article-category-link" href="/categories/Computer-Science/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="/categories/Computer-Science/Machine-Learning/Neural-Networks/">Neural Networks</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">2.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">14 min</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <h1 id="神经网络-学习笔记"><a href="#神经网络-学习笔记" class="headerlink" title="神经网络 学习笔记"></a>神经网络 学习笔记</h1><p>学习资源: <a href="">neuralnetworksanddeeplearning.com</a></p>
<h2 id="Chapter-1、2"><a href="#Chapter-1、2" class="headerlink" title="Chapter 1、2"></a>Chapter 1、2</h2><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h3><ul>
<li><p>activation function 激活函数/传递函数   定义了该节点在给定的输入或输入的集合下的输出，比如 sigmoid function</p>
</li>
<li><p>perceptron model 感知器：输入 $0,1$ 二值函数，输出 $0,1$ 二值函数</p>
</li>
<li><p>sigmoid neurons  S 神经元  </p>
<p>​    中间部分更精细，$\displaystyle\sigma(z)=\frac{1}{1+e^{-z}}$ 输入二值，输出为 $[0,1]$ 之间的数，极限性质和感知器一样</p>
</li>
<li><p>feedforward neural networks 前馈神经网络</p>
<p>​    没有反馈的神经网络，由输入层(input layer)、隐藏层(hidden layer)、输出层(output layer)构成</p>
</li>
</ul>
<p>$ \ $ <a id="more"></a></p>
<h3 id="1-2-stochastic-gradient-descent-随机梯度下降"><a href="#1-2-stochastic-gradient-descent-随机梯度下降" class="headerlink" title="1.2 stochastic gradient descent 随机梯度下降"></a>1.2 stochastic gradient descent 随机梯度下降</h3><h4 id="1-2-1-参数"><a href="#1-2-1-参数" class="headerlink" title="1.2.1 参数"></a>1.2.1 参数</h4><ul>
<li>$x$ ：输入的训练集（一个矩阵，每一列是一次单独的输入）,  $y$ ：最终输出，$y=w\cdot x+b$</li>
<li>$w$：weights，由矩阵组成的向量。$w_j$ 是一个 $m\times n$ 型矩阵，连接 $j-1$ 层和 $j$ 层，其中 $j-1$ 层有 $n$ 个神经元，$ j $ 层有 $m$ 个神经元</li>
<li>$b$ ：biases，由 $ m\times 1$ 型矩阵组成的向量。 $b_k$ 是一个$m\times 1$ 型矩阵，连接 $j-1$ 层和 $j$ 层，其中第 $j$ 层有 $m$ 个神经元</li>
<li>Cost function ： $ C(w,b)=\displaystyle\frac{1}{2n}\sum<em>x |y(x)-a |^2=\frac{1}{2n}\sum</em>{i=1}^{n}[y(x^{(i)})-a^{(i)}]^2 $</li>
</ul>
<h4 id="1-2-2-方法"><a href="#1-2-2-方法" class="headerlink" title="1.2.2 方法"></a>1.2.2 方法</h4><ul>
<li>对于整个训练集，用随机梯度下降法使损失函数尽可能小</li>
<li>先将训练集打乱，再按给定的小批训练集大小等间隔取小批训练集（相当于随机取样），每次选择更优的 $w,b$ 使损失函数一步步减小</li>
</ul>
<h4 id="1-2-3-保证每次都获得更优的-w-b-："><a href="#1-2-3-保证每次都获得更优的-w-b-：" class="headerlink" title="1.2.3 保证每次都获得更优的 $w,b$："></a>1.2.3 保证每次都获得更优的 $w,b$：</h4><ol>
<li><script type="math/tex; mode=display">
\displaystyle \nabla C=(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T , \quad \vec v_1\equiv v_1\equiv w,\vec v_2\equiv v_2\equiv b\\ \displaystyle\Delta C\approx  \frac{\partial C}{\partial w}\Delta v_1+\frac{\partial C}{\partial b}\Delta v_2=\Delta v\cdot \nabla C \\ 
=> take\quad \Delta v=(\Delta v_1,\Delta v_2)=-\eta \nabla C,\quad \eta>0\\
\begin{aligned}
\therefore C&:=C+\Delta C=C-\eta \|\nabla C \|^2\\
v_1& :=v_1-\eta\cdot\frac{\partial C}{\partial v_1}\\
v_2&:=v_2-\eta\cdot\frac{\partial C}{\partial v_2s}
\end{aligned}</script></li>
<li><p>分量式：</p>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
w_{jk}&:=w_{jk}-\eta\cdot \frac{\partial C}{\partial w_{jk}}=w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial w_{jk}^{(i)}} \\
b_{k}&:=b_{ k}-\eta\cdot \frac{\partial C}{\partial b_{k}}=b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial b_{k}^{(i)}}
\end{aligned}</script><h2 id="2-BP算法（反向传播-back-propagate-）"><a href="#2-BP算法（反向传播-back-propagate-）" class="headerlink" title="2 BP算法（反向传播 back propagate ）"></a>2 BP算法（反向传播 back propagate ）</h2><p>目的：快速计算 $\displaystyle \frac{\partial C}{\partial w},\frac{\partial C}{\partial b}$ .</p>
<h3 id="2-1-原理："><a href="#2-1-原理：" class="headerlink" title="2.1 原理："></a>2.1 原理：</h3><p>最终结果产生的偏差是由于每个神经元产生的误差叠加起来造成的。通过前向反馈得到的结果若与正确结果有偏差，则从此偏差反向推导，可得到各层神经元产生的误差。</p>
<h3 id="2-2-符号表示"><a href="#2-2-符号表示" class="headerlink" title="2.2 符号表示"></a>2.2 符号表示</h3><ul>
<li>$w_{jk}^l$ ：连接第 $l-1$层第 $k$ 个神经元和第 $l$ 层第 $j$ 个神经元的权重</li>
<li>$w^l$ ：连接第 $l-1$ 和第 $l$ 层网络的权重矩阵</li>
<li>$b_j^l$ ：第 $l$ 层第 $j$ 个神经元的 bias</li>
<li>$z<em>j^l$ ：第 $l$ 层网络的带权输入  $z_j^l=\displaystyle\sum_k w</em>{jk}^l a_k^{l-1}+b_j^l$，$z^l=w^l a^{l-1}+b^l$</li>
<li><p>$\delta_j^l$ ：第 $l$ 层第 $j$ 个神经元产生的误差,，定义 $\displaystyle \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}$</p>
</li>
<li><p>$a_j^l$ ：第 $l$ 层网络的第 $j$ 个神经元的激活量，$a_j^l=\sigma(z_j^l)$</p>
</li>
<li>$C$ ：损失函数，用小批量输入的平均值近似。</li>
</ul>
<h3 id="2-3-重要的关系公式"><a href="#2-3-重要的关系公式" class="headerlink" title="2.3 重要的关系公式"></a>2.3 重要的关系公式</h3><ol>
<li><p>注：推导中的除法 ‘’$\bigg /$ ‘’、乘法 $\odot$ （ Hadamard Product）均为<strong>elementwise</strong> </p>
</li>
<li><script type="math/tex; mode=display">
C=\displaystyle \frac{1}{2n}\sum_x \|y(x)-a^L(x)\|^2=\frac{1}{n}\sum_x ^n C(x)\approx\frac{1}{m}\sum_x^m C(x)
\\ C(x)=\frac{1}{2}\|y-a^L(x)\|^2=\frac{1}{2}\sum_i \big(y_i-a_i^L(x)\big)^2</script></li>
</ol>
<ol>
<li><script type="math/tex; mode=display">
a_j^l=\sigma\bigg(\sum_k w_{jk}^l a_k^{l-1}+b_j^l\bigg)
\\ a^l=\sigma\bigg(w^l a^{l-1}+b^l\bigg)</script></li>
</ol>
<ol>
<li><script type="math/tex; mode=display">
\delta^l=\frac{\partial C}{\partial b^l}\bigg/ \frac{\partial z^l}{\partial b^l}=\frac{\partial C}{\partial b^l},\qquad \frac{\partial C}{\partial b^l}\triangleq(\frac{\partial C}{\partial b_1^l},\cdots,\frac{\partial C}{\partial b_{N(l)}^l})^T \tag{1}</script></li>
</ol>
<ol>
<li><script type="math/tex; mode=display">
\delta_j^l=\frac{\partial C}{\partial w_{jk}^l}\bigg /\frac{\partial z^l}{\partial w_{jk}^l}=\frac{\partial C}{\partial w_{jk}^l}\big/ a_k^{l-1}</script></li>
</ol>
<script type="math/tex; mode=display">
i .e. \qquad \frac{\partial C}{\partial w_{jk}^l}=a_{k}^{l-1}\cdot  \delta_j^l \qquad =>\nabla C_{w^l}=\delta^l\cdot (a^{l-1})^T \tag{2}</script><ol>
<li><script type="math/tex; mode=display">
\delta^L=\frac{\partial C}{\partial a^L}\odot \frac{\partial a^L}{\partial z^L}=\frac{\partial C}{\partial a^L}\odot\sigma'(z^L)=(a^L-y) \odot \sigma'(z^L)\tag{3}</script></li>
</ol>
<ol>
<li><p><strong>Move error backward:</strong></p>
<script type="math/tex; mode=display">
\begin{eqnarray}
z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k
=>\frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j)
\end{eqnarray}</script><script type="math/tex; mode=display">
\delta_j^l=\sum_k \frac{\partial C}{\partial z_k^{l+1}}\cdot \frac{\partial z_k^{l+1}}{\partial z_j^l}=\sum_{k=1}^{N(l)}\delta_k^{l+1}\cdot w_{kj}^{l+1}\cdot \sigma'(z_j^l)</script></li>
</ol>
<script type="math/tex; mode=display">
   => \delta^l=\big((w^{l+1})^T\sigma'(z^l)\big)\odot \delta^{l+1}\tag{4}</script><h3 id="2-4-实现步骤"><a href="#2-4-实现步骤" class="headerlink" title="2.4 实现步骤"></a>2.4 实现步骤</h3><ol>
<li><p>feedforward :  得到 $z^L$ 和相应的 $a^L$</p>
<script type="math/tex; mode=display">
a^{x,l} = \sigma(z^{x,l})</script></li>
<li><p>output error :</p>
<script type="math/tex; mode=display">
\delta^{x,L} = \nabla_a C_x \odot \sigma'(z^{x,L})</script></li>
<li><p>error back propagate :  </p>
<script type="math/tex; mode=display">
\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1})
  \odot \sigma'(z^{x,l})</script></li>
</ol>
<ol>
<li>update weights and biases : <script type="math/tex; mode=display">
\begin{aligned}
w_{jk}&:=w_{jk}-\eta\cdot \nabla C_{w_{jk}}\approx w_{jk}-\eta\cdot \frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial w_{jk}^{(x_i)}}=w_{jk}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \big(a_k^{l-1}\delta_j^{l}\big)^{(x_i)}\\
b_{k}&:=b_{k}-\eta\cdot \nabla C_{b_{k}}\approx b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \frac{\partial C}{\partial b_{k}^{(x_i)}}=b_{k}-\eta\cdot\frac{1}{m}\sum_{i=1}^m \big(\delta_k^{l}\big)^{(x_i)}
\end{aligned}</script></li>
</ol>
<p>   即：</p>
<script type="math/tex; mode=display">
   w^l \rightarrow
     w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T
    \\b^l \rightarrow b^l-\frac{\eta}{m}
     \sum_x \delta^{x,l}</script><h3 id="2-5-代码实现"><a href="#2-5-代码实现" class="headerlink" title="2.5 代码实现"></a>2.5 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span><span class="hljs-params">(object)</span>:</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, sizes)</span>:</span><br>        <span class="hljs-comment"># 生成神经网络 </span><br>        self.num_layers = len(sizes)<br>        self.sizes = sizes<br>        self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> sizes[<span class="hljs-number">1</span>:]]<br>        self.weights = [np.random.randn(y, x)<br>                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(sizes[:<span class="hljs-number">-1</span>], sizes[<span class="hljs-number">1</span>:])]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">feedforward</span><span class="hljs-params">(self, a)</span>:</span><br>        <span class="hljs-comment"># 激活神经元，向后面的层产生反馈</span><br>        <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):<br>            a = sigmoid(np.dot(w, a) + b)<br>        <span class="hljs-keyword">return</span> a<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SGD</span><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta,<br>            test_data=None)</span>:</span><br>        <span class="hljs-comment"># 随机梯度下降 Stochastic Gradient Descent</span><br>        <span class="hljs-comment"># epochs 训练次数, mini_batch_size 每次选取的小训练集大小, eta 学习率 由自己设定</span><br>        <span class="hljs-keyword">if</span> test_data:<br>            test_data = list(test_data)  <br>            n_test = len(test_data)<br>        training_data = list(training_data)<br>        n = len(training_data)<br>        <br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(epochs):<br>            random.shuffle(training_data)<br>            mini_batches = [<br>                training_data[k:k + mini_batch_size]<br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, n, mini_batch_size)]  <br>            <span class="hljs-keyword">for</span> mini_batch <span class="hljs-keyword">in</span> mini_batches:<br>                self.update_mini_batch(mini_batch, eta)<br>            <span class="hljs-keyword">if</span> test_data:<br>                print(<span class="hljs-string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(j,<br>                                                    self.evaluate(test_data), n_test))<br><br>            <span class="hljs-keyword">else</span>:<br>                print(<span class="hljs-string">"Epoch &#123;0&#125; complete"</span>.format(j))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_mini_batch</span><span class="hljs-params">(self, mini_batch, eta)</span>:</span><br>        <span class="hljs-comment"># 更新 w 和 b</span><br>        nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]<br>        nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]<br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> mini_batch:<br>            delta_nabla_b, delta_nabla_w = self.backprop(x, y)<br>            nabla_b = [nb + dnb <span class="hljs-keyword">for</span> nb, dnb <span class="hljs-keyword">in</span> zip(nabla_b, delta_nabla_b)]<br>            nabla_w = [nw + dnw <span class="hljs-keyword">for</span> nw, dnw <span class="hljs-keyword">in</span> zip(nabla_w, delta_nabla_w)]<br>        self.weights = [w - (eta / len(mini_batch)) * nw<br>                        <span class="hljs-keyword">for</span> w, nw <span class="hljs-keyword">in</span> zip(self.weights, nabla_w)]<br>        self.biases = [b - (eta / len(mini_batch)) * nb<br>                       <span class="hljs-keyword">for</span> b, nb <span class="hljs-keyword">in</span> zip(self.biases, nabla_b)]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backprop</span><span class="hljs-params">(self, x, y)</span>:</span><br>        <span class="hljs-comment"># 反向传播算法</span><br>        nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]<br>        nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]<br>        <span class="hljs-comment"># feedforward</span><br>        activation = x<br>        activations = [x]  <span class="hljs-comment"># list to store all the activations, layer by layer</span><br>        zs = []  <span class="hljs-comment"># list to store all the z vectors, layer by layer</span><br>        <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):<br>            z = np.dot(w, activation) + b<br>            zs.append(z)<br>            activation = sigmoid(z)<br>            activations.append(activation)<br>        <span class="hljs-comment"># backward pass</span><br>        delta = self.cost_derivative(activations[<span class="hljs-number">-1</span>], y) * \<br>            sigmoid_prime(zs[<span class="hljs-number">-1</span>])<span class="hljs-comment"># \delta^L</span><br>        nabla_b[<span class="hljs-number">-1</span>] = delta<br>        nabla_w[<span class="hljs-number">-1</span>] = np.dot(delta, activations[<span class="hljs-number">-2</span>].transpose())<br><br>        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>, self.num_layers):<br>            z = zs[-l]<br>            sp = sigmoid_prime(z)<br>            delta = np.dot(self.weights[-l + <span class="hljs-number">1</span>].transpose(), delta) * sp<br>            nabla_b[-l] = delta<br>            nabla_w[-l] = np.dot(delta, activations[-l - <span class="hljs-number">1</span>].transpose())<br>        <span class="hljs-keyword">return</span> (nabla_b, nabla_w)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span><span class="hljs-params">(self, test_data)</span>:</span><br>       <span class="hljs-comment"># 返回输出结果和训练数据集的正确结果相同的个数</span><br>        test_results = [(np.argmax(self.feedforward(x)), y)<br>                        <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> test_data]<br>        <span class="hljs-keyword">return</span> sum(int(x == y) <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> test_results) <br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost_derivative</span><span class="hljs-params">(self, output_activations, y)</span>:</span><br>        <span class="hljs-string">"""Return the vector of partial derivatives \partial C_x /<br>        \partial a for the output activations."""</span><br>        <span class="hljs-keyword">return</span> (output_activations - y)<br><br><span class="hljs-comment"># Miscellaneous functions</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span><br>    <span class="hljs-comment"># The sigmoid function.</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + np.exp(-z))<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_prime</span><span class="hljs-params">(z)</span>:</span><br>    <span class="hljs-comment"># Derivative of the sigmoid function.</span><br>    <span class="hljs-keyword">return</span> sigmoid(z) * (<span class="hljs-number">1</span> - sigmoid(z))<br></code></pre></td></tr></table></figure>
<h2 id="Chapter-3"><a href="#Chapter-3" class="headerlink" title="Chapter 3"></a>Chapter 3</h2><h3 id="3-1-交叉熵"><a href="#3-1-交叉熵" class="headerlink" title="3.1 交叉熵"></a>3.1 交叉熵</h3><p>对错误的乘法力度加大，学习速率更快</p>
<h3 id="3-2-Softmax"><a href="#3-2-Softmax" class="headerlink" title="3.2 Softmax"></a>3.2 Softmax</h3><p>输出转化为概略分布</p>
<h3 id="3-3-其他损失函数"><a href="#3-3-其他损失函数" class="headerlink" title="3.3 其他损失函数"></a>3.3 其他损失函数</h3><h3 id="3-4-调整超参数的方法"><a href="#3-4-调整超参数的方法" class="headerlink" title="3.4 调整超参数的方法"></a>3.4 调整超参数的方法</h3><h2 id="Chapter-4"><a href="#Chapter-4" class="headerlink" title="Chapter 4"></a>Chapter 4</h2><p>神经网络可以计算任何函数。</p>
<h2 id="Chapter-5"><a href="#Chapter-5" class="headerlink" title="Chapter 5"></a>Chapter 5</h2><ul>
<li><p>vanishing gradient</p>
</li>
<li><p>exploding gradient</p>
</li>
</ul>
<h2 id="Chapter-6-CNN"><a href="#Chapter-6-CNN" class="headerlink" title="Chapter 6 CNN"></a>Chapter 6 CNN</h2><h3 id="6-1-架构"><a href="#6-1-架构" class="headerlink" title="6.1 架构"></a>6.1 架构</h3><ul>
<li>卷积层<ul>
<li>共享权重</li>
<li>提取特征</li>
</ul>
</li>
<li>池化层<ul>
<li>减少参量</li>
</ul>
</li>
<li>全连层<ul>
<li>二维压缩成一维，方便输出</li>
</ul>
</li>
<li>Softmax层<ul>
<li>输出概率分布</li>
</ul>
</li>
</ul>
<h3 id="6-2-代码：-with-pytorch"><a href="#6-2-代码：-with-pytorch" class="headerlink" title="6.2 代码： with pytorch"></a>6.2 代码： with pytorch</h3><h4 id="6-2-1-数据载入"><a href="#6-2-1-数据载入" class="headerlink" title="6.2.1 数据载入"></a>6.2.1 数据载入</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = torchvision.datasets.MNIST(<br>    root=<span class="hljs-string">'./'</span>,    <span class="hljs-comment"># 保存或者提取位置</span><br>    train=<span class="hljs-literal">True</span>, <br>    <span class="hljs-comment"># 转换 PIL.Image or numpy.ndarray</span><br>    transform=torchvision.transforms.ToTensor(),<br>    download=DOWNLOAD_MNIST,<br>)<br><span class="hljs-comment"># print(train_data.data.shape)</span><br><span class="hljs-comment">#train_data.data : (60000,28,28)</span><br><span class="hljs-comment"># train_data.targets : (60000)</span><br>test_data = torchvision.datasets.MNIST(root=<span class="hljs-string">'./'</span>, train=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># test_data 包括：test_data.data 和 test_data.targets</span><br><span class="hljs-comment">#test_data.data : (10000,28,28)</span><br><span class="hljs-comment">#test_data.targets : (10000)</span><br><br><span class="hljs-comment"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span><br>train_batches = Data.DataLoader(<br>    dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># print(train_batches.dataset.targets.shape</span><br><br><span class="hljs-comment"># 测试前2000个</span><br>test_x = torch.unsqueeze(test_data.data, dim=<span class="hljs-number">1</span>).type(torch.FloatTensor)[<br>    :<span class="hljs-number">2000</span>] / <span class="hljs-number">255.0</span>  <span class="hljs-comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span><br>test_y = test_data.targets[:<span class="hljs-number">2000</span>]<br></code></pre></td></tr></table></figure>
<h4 id="6-2-2-卷积神经网络构建"><a href="#6-2-2-卷积神经网络构建" class="headerlink" title="6.2.2 卷积神经网络构建"></a>6.2.2 卷积神经网络构建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CNN</span><span class="hljs-params">(nn.Module)</span>:</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span><br>        super(CNN, self).__init__()<br>        self.conv1 = nn.Sequential(  <span class="hljs-comment"># input shape (1, 28, 28)</span><br>            nn.Conv2d(<br>                in_channels=<span class="hljs-number">1</span>,      <span class="hljs-comment"># input height</span><br>                out_channels=<span class="hljs-number">16</span>,    <span class="hljs-comment"># n_filters i.e. number of features</span><br>                kernel_size=<span class="hljs-number">5</span>,      <span class="hljs-comment"># filter size</span><br>                stride=<span class="hljs-number">1</span>,           <span class="hljs-comment"># filter movement/step</span><br>                <span class="hljs-comment"># 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1</span><br>                padding=<span class="hljs-number">2</span>,<br>            ),      <span class="hljs-comment"># -&gt; (16, 28, 28)</span><br>            nn.ReLU(),    <span class="hljs-comment"># activation</span><br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>),    <span class="hljs-comment"># -&gt; (16, 14, 14)</span><br>        )<br>        self.conv2 = nn.Sequential(  <span class="hljs-comment"># input shape (16, 14, 14)</span><br>            nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),  <span class="hljs-comment"># output shape (32, 14, 14)</span><br>            nn.ReLU(),  <span class="hljs-comment"># activation</span><br>            nn.MaxPool2d(<span class="hljs-number">2</span>),  <span class="hljs-comment"># output shape (32, 7, 7)</span><br>        )<br>        <span class="hljs-comment"># fully connected layer, output 10 classes</span><br>        self.fc1 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">100</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span><br>        x = self.conv1(x)<br>        x = self.conv2(x)<br>        x = x.view(x.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)   <span class="hljs-comment"># 展平多维的卷积图成 (batch_size, 32 * 7 * 7)</span><br>        x = torch.sigmoid(self.fc1(x))<br>        x = self.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h4 id="6-2-3-反向传播、训练神经网络"><a href="#6-2-3-反向传播、训练神经网络" class="headerlink" title="6.2.3 反向传播、训练神经网络"></a>6.2.3 反向传播、训练神经网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">cnn = CNN()<br>optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) <span class="hljs-comment">#adam算法</span><br>loss_func = nn.CrossEntropyLoss()  <span class="hljs-comment"># Softmax–Log–NLLLoss，自动输出log-softmax</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epochs):<br>        <span class="hljs-comment"># 分配 batch data, normalize x when iterate train_loader</span><br>        <span class="hljs-keyword">for</span> step, (b_x, b_y) <span class="hljs-keyword">in</span> enumerate(train_batches):  <span class="hljs-comment"># enumerate 添加索引可便历</span><br>            <span class="hljs-comment"># print(b_x.shape)--&gt;(50,1,28,28)</span><br>            <span class="hljs-comment"># print(b_y.shape)--&gt;(50)</span><br>            <span class="hljs-comment"># 1200组</span><br>            output = cnn(b_x)               <span class="hljs-comment"># cnn output</span><br>            loss = loss_func(output, b_y)   <span class="hljs-comment"># cross entropy loss</span><br>            optimizer.zero_grad()           <span class="hljs-comment"># clear gradients for this training step</span><br>            loss.backward()                 <span class="hljs-comment"># backpropagation, compute gradients</span><br>            optimizer.step()                <span class="hljs-comment"># apply gradients</span><br>            <span class="hljs-keyword">if</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                test_output = cnn(test_x)<br>                pred_y = torch.max(test_output, <span class="hljs-number">1</span>)[<br>                    <span class="hljs-number">1</span>].data.numpy()  <span class="hljs-comment"># [0]:一行中的最大值,[1]:最大值的索引</span><br>                accuracy = float((pred_y == test_y.data.numpy()).astype(<br>                    int).sum()) / float(test_y.size(<span class="hljs-number">0</span>))<br>                print(<span class="hljs-string">'Epoch: '</span>, epoch, <span class="hljs-string">'| train loss: %.4f'</span> %<br>                      loss.data.numpy(), <span class="hljs-string">'| test accuracy: %.2f'</span> % accuracy)<br>    torch.save(cnn.state_dict(), <span class="hljs-string">"cnn_net_params.pkl"</span>)<br><br>train()<br></code></pre></td></tr></table></figure>
<h4 id="6-2-4-参数存储与加载"><a href="#6-2-4-参数存储与加载" class="headerlink" title="6.2.4 参数存储与加载"></a>6.2.4 参数存储与加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(cnn.state_dict(), <span class="hljs-string">"cnn_net_params.pkl"</span>)<span class="hljs-comment">#保存训练好的参数</span><br><br><br>cnn = CNN()<br>cnn.load_state_dict(torch.load(<span class="hljs-string">'cnn_net_params.pkl'</span>))<span class="hljs-comment">#加载参数</span><br></code></pre></td></tr></table></figure>
<p>其他：关于深度学习和图像处理卷积的定义问题：深度学习中conv2d卷积层其实是无所谓是否翻转的，因为所有的weights也就是kernel其实是随机初始化的。那么每次的更新迭代都是为了去寻找一个最合适的kernel，所以是否翻转也变的无关紧要了。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://longqianh.com/2020/01/30/Computer-Sciencs/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/notes/" rel="tag">notes</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/02/01/Mathematics/Mathematic-Models/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Mathematics Models
          
        </div>
      </a>
    
    
      <a href="/2020/01/28/Computer-Sciencs/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">数值分析 学习笔记</div>
      </a>
    
  </nav>


  

  

  
  
  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020
        <i class="ri-heart-fill heart_icon"></i> Longqian Huang
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/logo.png" alt="Peter&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/Notes">Notes</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/Projects">Projects</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/images/A%20Fancy%20Man.png">关于我</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="http://c01dkit.com" target="_blank" rel="noopener">友链</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i></p>
  <div class="reward-box">
    
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Subtitle -->

<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['Mathematics, Computer-Science, Physics.', 'Now I will scroll . . .', ''],
      startDelay: 0,
      typeSpeed: 70,
      loop: 10,
      backSpeed: false,
      showCursor: true
    });
  } catch (err) {
    console.log(err)
  }
</script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->


<script src="/js/clickLove.js"></script>


<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>



    
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>